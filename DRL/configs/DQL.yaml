name: "AdaptiveSampling"
#seed: 1

data_train_path: "/pub/panwangc/custom_DRL/JBHI_2022/dataset/to_Luke.pickle"
data_eval_path: "/pub/panwangc/custom_DRL/JBHI_2022/dataset/MIT-BIH_True-Final.npz"

policy_model_save_path: "/pub/panwangc/custom_DRL/JBHI_2022/AdaptiveSampling-DRL/DRL/model/"

use_PER: False
min_beta: 0.4
max_beta: 1.0

## Experiment
experiments_save_path: "/pub/panwangc/custom_DRL/JBHI_2022/AdaptiveSampling-DRL/DRL/experiments/"

num_trials: 5
experiment_num: 1

# Environment parameters
override_num_subjects: False
num_subjects_train: 1

normalize_rewards: True   # max, min
standardize_rewards: False    # mean, stdev

use_gpu: True
train_test_split: 0.8
mix_clips: False
gradient_clip: False
hard_update: False

# DDQN parameters
feature_size: 256
num_actions: 4
max_epsilon: 1.0 # determines the action for exploration vs exploitation
min_epsilon: 0.01
epsilon_decay_rate: 1
batch_delay: 1
replay_batch_size: 256
learning_rate_max: 0.003
learning_rate_min: 0.00001
gamma: 0.99    # the discount value
target_network_hard_update_rate: 1000   # target network updates every n episodes for hard update
tau: 0.1    # used for softupdate
replay_memory_size: 1000000