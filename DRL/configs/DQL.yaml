name: "AdaptiveSamplingV3"
#seed: 1

data_train_path: "" # specify the path to the training data pickle file
data_eval_path: "" # specify the path to the evaluation data pickle file

policy_model_save_path: "" # specify the path for saving trained models

use_PER: False
min_beta: 0.4
max_beta: 1.0

## Experiment
experiments_save_path: "" # specify the path for saving evaluation results

num_trials: 6
experiment_num: 1

# Environment parameters
override_num_subjects: False
num_subjects_train: 5

reward_lambda: 1
normalize_rewards: True   # max, min
standardize_rewards: False    # mean, stdev

use_gpu: True
gradient_clip: False

# DDQN parameters
feature_size: 256
num_actions: 4
max_epsilon: 1.0 # determines the action for exploration vs exploitation
min_epsilon: 0.01
epsilon_decay_rate: 1
batch_delay: 1
replay_batch_size: 8
learning_rate_max: 0.0001
learning_rate_min: 0.00001
gamma: 0.99    # the discount value
hard_update: True
target_network_hard_update_rate: 100   # target network updates every n episodes for hard update
tau: 0.001    # used for softupdate
replay_memory_size: 1000000